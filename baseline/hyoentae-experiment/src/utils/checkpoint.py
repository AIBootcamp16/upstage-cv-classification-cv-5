"""
ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì˜ ì €ì¥ê³¼ ë¡œë“œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
Best ëª¨ë¸ ì €ì¥, Early Stopping ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import torch
import os
from pathlib import Path
from typing import Dict, Any, Optional


class CheckpointManager:
    """
    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤

    Best ëª¨ë¸ ì €ì¥, ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥, Early Stopping ë“±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        >>> manager = CheckpointManager(save_dir="outputs", patience=10)
        >>> manager.save_checkpoint(model, optimizer, epoch, val_f1)
        >>> if manager.should_stop():
        >>>     print("Early stopping!")
    """

    def __init__(
        self,
        save_dir: str,
        metric_name: str = "macro_f1",
        mode: str = "max",
        patience: int = 10,
        verbose: bool = True,
        use_generalization_score: bool = True,
        overfitting_penalty: float = 0.3,
    ):
        """
        ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”

        Args:
            save_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
            metric_name: ì¶”ì í•  ë©”íŠ¸ë¦­ ì´ë¦„ (ì˜ˆ: "macro_f1")
            mode: ë©”íŠ¸ë¦­ ëª¨ë“œ ("max" ë˜ëŠ” "min")
                 - "max": ê°’ì´ í´ìˆ˜ë¡ ì¢‹ìŒ (accuracy, f1 ë“±)
                 - "min": ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ (loss ë“±)
            patience: Early stopping patience (ëª‡ epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)
            verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
            use_generalization_score: Generalization score ì‚¬ìš© ì—¬ë¶€
                 - True: Val ë©”íŠ¸ë¦­ - ê³¼ì í•© í˜ë„í‹°ë¡œ best íŒë‹¨
                 - False: Val ë©”íŠ¸ë¦­ë§Œìœ¼ë¡œ best íŒë‹¨
            overfitting_penalty: ê³¼ì í•© í˜ë„í‹° ê°€ì¤‘ì¹˜ (0.0~1.0)
                 - 0.0: ê³¼ì í•© ë¬´ì‹œ (Valë§Œ ê³ ë ¤)
                 - 0.3 (ê¶Œì¥): ì ë‹¹í•œ ê³¼ì í•© ë°©ì§€
                 - 0.5: ê°•í•œ ê³¼ì í•© ë°©ì§€
        """
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)

        self.metric_name = metric_name
        self.mode = mode
        self.patience = patience
        self.verbose = verbose
        self.use_generalization_score = use_generalization_score
        self.overfitting_penalty = overfitting_penalty

        # Best ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        if mode == "max":
            self.best_metric = float('-inf')
            self.best_score = float('-inf')  # Generalization score
            self.compare = lambda x, y: x > y
        else:  # mode == "min"
            self.best_metric = float('inf')
            self.best_score = float('inf')
            self.compare = lambda x, y: x < y

        # Early stopping ì¹´ìš´í„°
        self.counter = 0
        self.best_epoch = 0
        self.best_train_metric = None  # Best epochì˜ Train ë©”íŠ¸ë¦­

    def save_checkpoint(
        self,
        model: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        epoch: int,
        metric_value: float,
        train_metric_value: Optional[float] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        extra_info: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥

        ë©”íŠ¸ë¦­ì´ ê°œì„ ë˜ì—ˆìœ¼ë©´ best.pthë¡œ ì €ì¥í•˜ê³  True ë°˜í™˜,
        ì•„ë‹ˆë©´ False ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            model: PyTorch ëª¨ë¸
            optimizer: Optimizer
            epoch: í˜„ì¬ epoch
            metric_value: í˜„ì¬ Val ë©”íŠ¸ë¦­ ê°’
            train_metric_value: í˜„ì¬ Train ë©”íŠ¸ë¦­ ê°’ (Generalization score ê³„ì‚°ìš©, ì˜µì…˜)
            scheduler: Learning rate scheduler (ì˜µì…˜)
            extra_info: ì¶”ê°€ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ì˜µì…˜)

        Returns:
            Best ëª¨ë¸ì´ ê°±ì‹ ë˜ì—ˆìœ¼ë©´ True, ì•„ë‹ˆë©´ False

        ì˜ˆì‹œ:
            >>> is_best = manager.save_checkpoint(
            >>>     model, optimizer, epoch=10,
            >>>     metric_value=0.85, train_metric_value=0.90
            >>> )
            >>> if is_best:
            >>>     print("ìƒˆë¡œìš´ best ëª¨ë¸ ì €ì¥!")
        """
        # Generalization Score ê³„ì‚°
        if self.use_generalization_score and train_metric_value is not None:
            # ê³¼ì í•© í˜ë„í‹° ê³„ì‚°
            if self.mode == "max":
                # F1 ë“±: Trainì´ Valë³´ë‹¤ ë†’ìœ¼ë©´ ê³¼ì í•©
                gap = max(0, train_metric_value - metric_value)
                score = metric_value - self.overfitting_penalty * gap
            else:  # mode == "min"
                # Loss ë“±: Valì´ Trainë³´ë‹¤ ë†’ìœ¼ë©´ ê³¼ì í•©
                gap = max(0, metric_value - train_metric_value)
                score = metric_value + self.overfitting_penalty * gap

            # Score ê¸°ì¤€ìœ¼ë¡œ ê°œì„  ì—¬ë¶€ íŒë‹¨
            is_improved = self.compare(score, self.best_score)
            current_score = score
        else:
            # ê¸°ì¡´ ë°©ì‹: Val ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš©
            is_improved = self.compare(metric_value, self.best_metric)
            current_score = metric_value

        if is_improved:
            # Best ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.best_metric = metric_value
            self.best_score = current_score
            self.best_epoch = epoch
            self.best_train_metric = train_metric_value  # Best epochì˜ Train ë©”íŠ¸ë¦­ ì €ì¥
            self.counter = 0  # Early stopping ì¹´ìš´í„° ë¦¬ì…‹

            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì„±
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                self.metric_name: metric_value,
                'best_metric': self.best_metric,
            }

            # Train ë©”íŠ¸ë¦­ë„ ì €ì¥ (ìˆìœ¼ë©´)
            if train_metric_value is not None:
                checkpoint['train_' + self.metric_name] = train_metric_value
                checkpoint['generalization_score'] = current_score

            # Schedulerê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if scheduler is not None:
                checkpoint['scheduler_state_dict'] = scheduler.state_dict()

            # ì¶”ê°€ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if extra_info is not None:
                checkpoint.update(extra_info)

            # Best ëª¨ë¸ ì €ì¥
            best_path = self.save_dir / "best.pth"
            torch.save(checkpoint, best_path)

            if self.verbose:
                print(f"âœ… Best ëª¨ë¸ ì €ì¥: {self.metric_name}={metric_value:.4f} (Epoch {epoch})")
                if self.use_generalization_score and train_metric_value is not None:
                    gap = train_metric_value - metric_value if self.mode == "max" else metric_value - train_metric_value
                    print(f"   Generalization Score: {current_score:.4f}")
                    print(f"   Train-Val Gap: {gap:+.4f} (Penalty: {self.overfitting_penalty * max(0, gap):.4f})")
                print(f"   ì €ì¥ ê²½ë¡œ: {best_path}")

            return True

        else:
            # ë©”íŠ¸ë¦­ ê°œì„  ì•ˆ ë¨ -> Early stopping ì¹´ìš´í„° ì¦ê°€
            self.counter += 1

            if self.verbose:
                if self.use_generalization_score and train_metric_value is not None:
                    print(f"âš ï¸  ë©”íŠ¸ë¦­ ê°œì„  ì—†ìŒ: {self.metric_name}={metric_value:.4f}, Score={current_score:.4f} "
                          f"(Best: {self.best_metric:.4f}, Best Score: {self.best_score:.4f}, "
                          f"Patience: {self.counter}/{self.patience})")
                else:
                    print(f"âš ï¸  ë©”íŠ¸ë¦­ ê°œì„  ì—†ìŒ: {self.metric_name}={metric_value:.4f} "
                          f"(Best: {self.best_metric:.4f}, Patience: {self.counter}/{self.patience})")

            return False

    def save_last_checkpoint(
        self,
        model: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        epoch: int,
        metric_value: float,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    ):
        """
        ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (bestì™€ ë³„ê°œ)

        í•™ìŠµ ì¬ê°œë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ epochì˜ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            model: PyTorch ëª¨ë¸
            optimizer: Optimizer
            epoch: í˜„ì¬ epoch
            metric_value: í˜„ì¬ ë©”íŠ¸ë¦­ ê°’
            scheduler: Learning rate scheduler (ì˜µì…˜)
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            self.metric_name: metric_value,
        }

        if scheduler is not None:
            checkpoint['scheduler_state_dict'] = scheduler.state_dict()

        # last.pthë¡œ ì €ì¥
        last_path = self.save_dir / "last.pth"
        torch.save(checkpoint, last_path)

        if self.verbose:
            print(f"ğŸ’¾ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {last_path}")

    def should_stop(self) -> bool:
        """
        Early stopping ì—¬ë¶€ í™•ì¸

        Patienceë§Œí¼ epoch ë™ì•ˆ ë©”íŠ¸ë¦­ ê°œì„ ì´ ì—†ìœ¼ë©´ True ë°˜í™˜

        Returns:
            Early stopping í•´ì•¼ í•˜ë©´ True

        ì˜ˆì‹œ:
            >>> if manager.should_stop():
            >>>     print("Early stopping ë°œë™!")
            >>>     break
        """
        return self.counter >= self.patience

    def load_checkpoint(
        self,
        checkpoint_path: str,
        model: torch.nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        device: str = "cuda",
    ) -> Dict[str, Any]:
        """
        ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

        ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì½ì–´ì„œ ëª¨ë¸ê³¼ optimizerì— ì ìš©í•©ë‹ˆë‹¤.

        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            model: PyTorch ëª¨ë¸ (state_dictê°€ ë¡œë“œë¨)
            optimizer: Optimizer (ì˜µì…˜, state_dictê°€ ë¡œë“œë¨)
            scheduler: Scheduler (ì˜µì…˜, state_dictê°€ ë¡œë“œë¨)
            device: ë””ë°”ì´ìŠ¤ ("cuda" ë˜ëŠ” "cpu")

        Returns:
            ì²´í¬í¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬ (epoch, metric ë“± í¬í•¨)

        ì˜ˆì‹œ:
            >>> checkpoint = manager.load_checkpoint(
            >>>     "outputs/best.pth", model, optimizer, device="cuda"
            >>> )
            >>> print(f"Loaded epoch: {checkpoint['epoch']}")
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (PyTorch 2.6+ í˜¸í™˜ì„±)
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

        # ëª¨ë¸ state_dict ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])

        # Optimizer state_dict ë¡œë“œ (ìˆìœ¼ë©´)
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        # Scheduler state_dict ë¡œë“œ (ìˆìœ¼ë©´)
        if scheduler is not None and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        if self.verbose:
            epoch = checkpoint.get('epoch', 'unknown')
            metric_val = checkpoint.get(self.metric_name, 'unknown')
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
            print(f"   Epoch: {epoch}, {self.metric_name}: {metric_val}")

        return checkpoint

    def get_best_metric(self) -> float:
        """
        Best ë©”íŠ¸ë¦­ ê°’ ë°˜í™˜

        Returns:
            Best ë©”íŠ¸ë¦­ ê°’
        """
        return self.best_metric

    def get_best_epoch(self) -> int:
        """
        Best ë©”íŠ¸ë¦­ì„ ê¸°ë¡í•œ epoch ë°˜í™˜

        Returns:
            Best epoch ë²ˆí˜¸
        """
        return self.best_epoch

    def get_best_train_metric(self) -> Optional[float]:
        """
        Best epochì˜ Train ë©”íŠ¸ë¦­ ê°’ ë°˜í™˜

        Returns:
            Best epochì˜ Train ë©”íŠ¸ë¦­ ê°’ (ì—†ìœ¼ë©´ None)
        """
        return self.best_train_metric


def find_latest_checkpoint(output_dir: str = "outputs") -> str:
    """
    ê°€ì¥ ìµœê·¼ ì‹¤í—˜ì˜ best.pth ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.

    Args:
        output_dir: outputs ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        ìµœì‹  best.pth ê²½ë¡œ

    Raises:
        FileNotFoundError: best.pthë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ

    ì˜ˆì‹œ:
        >>> checkpoint_path = find_latest_checkpoint(output_dir="outputs")
        >>> print(checkpoint_path)  # outputs/2025-11-02/12-34-56/best.pth
    """
    import glob
    from pathlib import Path

    output_path = Path(output_dir)

    # outputs/**/best.pth íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  best.pth ì°¾ê¸°
    checkpoint_pattern = str(output_path / "**" / "best.pth")
    checkpoints = glob.glob(checkpoint_pattern, recursive=True)

    if not checkpoints:
        raise FileNotFoundError(
            f"âŒ {output_dir}/ ë””ë ‰í† ë¦¬ì—ì„œ best.pthë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            f"   ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•´ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”."
        )

    # íŒŒì¼ ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ì •ë ¬ (ê°€ì¥ ìµœê·¼ ê²ƒ ì„ íƒ)
    latest_checkpoint = max(checkpoints, key=lambda x: Path(x).stat().st_mtime)

    print(f"ğŸ” ìë™ ê²€ìƒ‰: ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë°œê²¬")
    print(f"   ê²½ë¡œ: {latest_checkpoint}")

    # í•´ë‹¹ ì‹¤í—˜ì˜ configë„ ì¶œë ¥
    checkpoint_dir = Path(latest_checkpoint).parent
    hydra_config = checkpoint_dir / ".hydra" / "config.yaml"
    if hydra_config.exists():
        import yaml
        with open(hydra_config, 'r') as f:
            config = yaml.safe_load(f)
            model_name = config.get('model', {}).get('name', 'unknown')
            aug_type = config.get('augmentation', {}).get('name', 'unknown')
            print(f"   ëª¨ë¸: {model_name}, ì¦ê°•: {aug_type}")

    return latest_checkpoint


def load_model_for_inference(
    model: torch.nn.Module,
    checkpoint_path: str,
    device: str = "cuda",
) -> torch.nn.Module:
    """
    ì¶”ë¡ ìš© ëª¨ë¸ ë¡œë“œ (ê°„ë‹¨ ë²„ì „)

    ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ë§Œ ë¡œë“œí•˜ê³  eval ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.

    Args:
        model: PyTorch ëª¨ë¸
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        device: ë””ë°”ì´ìŠ¤

    Returns:
        ë¡œë“œë˜ê³  eval ëª¨ë“œë¡œ ì„¤ì •ëœ ëª¨ë¸

    ì˜ˆì‹œ:
        >>> model = create_model(cfg)
        >>> model = load_model_for_inference(model, "outputs/best.pth", "cuda")
        >>> predictions = model(inputs)
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (PyTorch 2.6+ í˜¸í™˜ì„±)
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # ëª¨ë¸ state_dict ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])

    # Eval ëª¨ë“œë¡œ ì „í™˜
    model.eval()

    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    model = model.to(device)

    print(f"âœ… ì¶”ë¡ ìš© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")

    return model
