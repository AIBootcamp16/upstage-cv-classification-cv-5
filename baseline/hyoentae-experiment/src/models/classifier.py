"""
ëª¨ë¸ ë¶„ë¥˜ê¸° ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ timm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.

ì§€ì› ëª¨ë¸:
- EfficientNet (B0, B3)
- ResNet50
- ResNeXt50
- ConvNeXt Tiny
"""

import timm
import torch
import torch.nn as nn
from typing import Optional


class DocumentClassifier(nn.Module):
    """
    ë¬¸ì„œ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë¸ ë˜í¼

    timm ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ pre-trained ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    17ê°œ í´ë˜ìŠ¤ ë¬¸ì„œ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        >>> model = DocumentClassifier(
        >>>     model_name='efficientnet_b0',
        >>>     num_classes=17,
        >>>     pretrained=True,
        >>>     dropout=0.2
        >>> )
        >>> outputs = model(images)  # [batch_size, 17]
    """

    def __init__(
        self,
        model_name: str,
        num_classes: int = 17,
        pretrained: bool = True,
        dropout: float = 0.2,
        drop_path_rate: Optional[float] = None,
    ):
        """
        ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            model_name: timm ëª¨ë¸ ì´ë¦„
                       ì˜ˆ: 'efficientnet_b0', 'resnet50', 'convnext_tiny'
            num_classes: ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: 17)
            pretrained: ImageNet pre-trained ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
            dropout: Dropout ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)
            drop_path_rate: DropPath ë¹„ìœ¨ (ConvNeXt ë“±ì—ì„œ ì‚¬ìš©)
        """
        super(DocumentClassifier, self).__init__()

        self.model_name = model_name
        self.num_classes = num_classes

        # timm ëª¨ë¸ ìƒì„±
        if drop_path_rate is not None:
            # ConvNeXt ë“± DropPathë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸
            self.backbone = timm.create_model(
                model_name,
                pretrained=pretrained,
                num_classes=num_classes,
                drop_rate=dropout,
                drop_path_rate=drop_path_rate,
            )
        else:
            # ì¼ë°˜ ëª¨ë¸
            self.backbone = timm.create_model(
                model_name,
                pretrained=pretrained,
                num_classes=num_classes,
                drop_rate=dropout,
            )

        print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ: {model_name}")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
        print(f"   Pretrained: {pretrained}")
        print(f"   Dropout: {dropout}")
        if drop_path_rate is not None:
            print(f"   DropPath: {drop_path_rate}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass

        Args:
            x: ì…ë ¥ ì´ë¯¸ì§€ (shape: [batch_size, 3, H, W])

        Returns:
            ì¶œë ¥ logits (shape: [batch_size, num_classes])
        """
        return self.backbone(x)

    def get_num_parameters(self) -> int:
        """
        ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ë°˜í™˜

        Returns:
            íŒŒë¼ë¯¸í„° ê°œìˆ˜
        """
        return sum(p.numel() for p in self.parameters())

    def get_trainable_parameters(self) -> int:
        """
        í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ë°˜í™˜

        Returns:
            í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê°œìˆ˜
        """
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


def create_model_from_config(cfg, num_classes: int = 17) -> DocumentClassifier:
    """
    Hydra configë¡œë¶€í„° ëª¨ë¸ ìƒì„±

    configs/model/ í´ë”ì˜ YAML ì„¤ì •ì„ ì½ì–´ì„œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        cfg: Hydra config (cfg.model ì„¹ì…˜ í•„ìš”)
        num_classes: í´ë˜ìŠ¤ ìˆ˜

    Returns:
        DocumentClassifier ì¸ìŠ¤í„´ìŠ¤

    ì˜ˆì‹œ:
        >>> @hydra.main(config_path="configs", config_name="config")
        >>> def main(cfg):
        >>>     model = create_model_from_config(cfg, num_classes=17)
    """
    model_cfg = cfg.model

    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    model_name = model_cfg.get('name')
    pretrained = model_cfg.get('pretrained', True)
    dropout = model_cfg.get('dropout', 0.2)
    drop_path_rate = model_cfg.get('drop_path_rate', None)

    # ëª¨ë¸ ìƒì„±
    model = DocumentClassifier(
        model_name=model_name,
        num_classes=num_classes,
        pretrained=pretrained,
        dropout=dropout,
        drop_path_rate=drop_path_rate,
    )

    # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
    total_params = model.get_num_parameters()
    trainable_params = model.get_trainable_parameters()
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:")
    print(f"   ì „ì²´: {total_params:,}")
    print(f"   í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}")

    return model


def get_model_input_size(model_name: str) -> int:
    """
    ëª¨ë¸ì˜ ê¸°ë³¸ ì…ë ¥ í¬ê¸° ë°˜í™˜

    Args:
        model_name: timm ëª¨ë¸ ì´ë¦„

    Returns:
        ê¸°ë³¸ ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ì •ì‚¬ê°í˜•)

    ì˜ˆì‹œ:
        >>> img_size = get_model_input_size('efficientnet_b0')
        >>> print(img_size)  # 224
    """
    # timmì˜ ê¸°ë³¸ ì…ë ¥ í¬ê¸°
    default_sizes = {
        'efficientnet_b0': 224,
        'efficientnet_b3': 300,
        'resnet50': 224,
        'resnext50_32x4d': 224,
        'convnext_tiny': 224,
    }

    return default_sizes.get(model_name, 224)


class ModelEMA:
    """
    Exponential Moving Average (EMA) ëª¨ë¸

    í•™ìŠµ ì¤‘ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ì´ë™ í‰ê· ì„ ìœ ì§€í•˜ì—¬
    ë” ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        >>> model = create_model(cfg)
        >>> ema = ModelEMA(model, decay=0.9999)
        >>> # í•™ìŠµ ë£¨í”„
        >>> for batch in train_loader:
        >>>     loss = train_step(batch)
        >>>     loss.backward()
        >>>     optimizer.step()
        >>>     ema.update(model)  # EMA ì—…ë°ì´íŠ¸
        >>> # í‰ê°€ ì‹œ
        >>> ema.apply_shadow()  # EMA íŒŒë¼ë¯¸í„°ë¡œ êµì²´
        >>> evaluate(model)
        >>> ema.restore()  # ì›ë˜ íŒŒë¼ë¯¸í„°ë¡œ ë³µì›
    """

    def __init__(self, model: nn.Module, decay: float = 0.9999):
        """
        EMA ì´ˆê¸°í™”

        Args:
            model: PyTorch ëª¨ë¸
            decay: EMA decay ë¹„ìœ¨ (0.999 ~ 0.9999 ê¶Œì¥)
        """
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}

        # ì´ˆê¸° shadow íŒŒë¼ë¯¸í„° ìƒì„±
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self, model: nn.Module):
        """
        EMA íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

        Args:
            model: í˜„ì¬ í•™ìŠµ ì¤‘ì¸ ëª¨ë¸
        """
        for name, param in model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        """
        ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ EMA íŒŒë¼ë¯¸í„°ë¡œ êµì²´
        (í‰ê°€ ì „ì— í˜¸ì¶œ)
        """
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        """
        ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
        (í‰ê°€ í›„ì— í˜¸ì¶œ)
        """
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}
