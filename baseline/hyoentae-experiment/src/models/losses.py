"""
Loss í•¨ìˆ˜ ëª¨ë“ˆ

ë‹¤ì–‘í•œ Loss í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤:
- FocalLoss: ì–´ë ¤ìš´ ìƒ˜í”Œì— ìë™ ì§‘ì¤‘
- AsymmetricLoss: False Positive ê°ì†Œ
- LabelSmoothingCrossEntropy: ê³¼ì í•© ë°©ì§€

í´ë˜ìŠ¤ 3-7, 14ë²ˆ ê°™ì€ ì–´ë ¤ìš´ í´ë˜ìŠ¤ ëŒ€ì‘ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class FocalLoss(nn.Module):
    """
    Focal Loss (Lin et al., 2017) - ì–´ë ¤ìš´ ìƒ˜í”Œì— ìë™ ì§‘ì¤‘!

    Focal Loss = -alpha * (1 - p_t)^gamma * log(p_t)

    where:
        - p_t: ì •ë‹µ í´ë˜ìŠ¤ì˜ ì˜ˆì¸¡ í™•ë¥ 
        - alpha: í´ë˜ìŠ¤ ê· í˜• íŒŒë¼ë¯¸í„° (ë³´í†µ 0.25)
        - gamma: focusing íŒŒë¼ë¯¸í„° (ë³´í†µ 2.0)

    gammaê°€ í´ìˆ˜ë¡ ì–´ë ¤ìš´ ìƒ˜í”Œ(ë‚®ì€ í™•ë¥ )ì— ë” ì§‘ì¤‘í•©ë‹ˆë‹¤:
    - gamma=0: Cross Entropyì™€ ë™ì¼
    - gamma=2: í™•ë¥  0.9ì¸ ìƒ˜í”Œì˜ lossê°€ 0.01ë°°ë¡œ ê°ì†Œ
    - gamma=5: í™•ë¥  0.9ì¸ ìƒ˜í”Œì˜ lossê°€ 0.00001ë°°ë¡œ ê°ì†Œ

    í´ë˜ìŠ¤ 3-7, 14ì²˜ëŸ¼ ì–´ë ¤ìš´ í´ë˜ìŠ¤ì— ìë™ìœ¼ë¡œ ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤!

    Args:
        alpha: í´ë˜ìŠ¤ ê· í˜• íŒŒë¼ë¯¸í„° (0~1, ê¸°ë³¸ê°’: 0.25)
        gamma: Focusing íŒŒë¼ë¯¸í„° (0~5, ê¸°ë³¸ê°’: 2.0)
        reduction: 'mean' ë˜ëŠ” 'sum'
        label_smoothing: Label smoothing ë¹„ìœ¨ (0~1, ê¸°ë³¸ê°’: 0.0)
        weight: í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ (Optional[Tensor])

    ì˜ˆì‹œ:
        >>> # í´ë˜ìŠ¤ 3-7, 14ì— ì§‘ì¤‘í•˜ë ¤ë©´ gammaë¥¼ ë†’ê²Œ
        >>> criterion = FocalLoss(alpha=0.25, gamma=3.0, label_smoothing=0.1)
        >>> loss = criterion(outputs, labels)
    """

    def __init__(
        self,
        alpha: float = 0.25,
        gamma: float = 2.0,
        reduction: str = 'mean',
        label_smoothing: float = 0.0,
        weight: Optional[torch.Tensor] = None,
    ):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.label_smoothing = label_smoothing
        self.weight = weight

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: ëª¨ë¸ ì¶œë ¥ logits (shape: [batch_size, num_classes])
            targets: ì •ë‹µ ë ˆì´ë¸” (shape: [batch_size])

        Returns:
            Focal loss (scalar)
        """
        # Cross Entropy Loss with Label Smoothing
        ce_loss = F.cross_entropy(
            inputs,
            targets,
            reduction='none',
            label_smoothing=self.label_smoothing,
            weight=self.weight,
        )

        # p_t ê³„ì‚° (ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ )
        p = torch.exp(-ce_loss)  # p = exp(-ce_loss)

        # Focal Loss ê³„ì‚°
        # (1 - p)^gamma í•­ì´ ì–´ë ¤ìš´ ìƒ˜í”Œì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬
        focal_loss = self.alpha * (1 - p) ** self.gamma * ce_loss

        # Reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class AsymmetricLoss(nn.Module):
    """
    Asymmetric Loss (Ridnik et al., 2021) - Multi-label classificationì— íš¨ê³¼ì 

    Positiveì™€ Negative ìƒ˜í”Œì— ë‹¤ë¥¸ gamma ì ìš©:
    - Positive: ì–´ë ¤ìš´ ìƒ˜í”Œ (gamma_pos)
    - Negative: ì‰¬ìš´ ìƒ˜í”Œ (gamma_neg)

    ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œ False Positiveë¥¼ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤.

    Args:
        gamma_pos: Positive ìƒ˜í”Œì˜ focusing íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 0)
        gamma_neg: Negative ìƒ˜í”Œì˜ focusing íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 4)
        clip: Negative probability clipping (ê¸°ë³¸ê°’: 0.05)

    ì˜ˆì‹œ:
        >>> criterion = AsymmetricLoss(gamma_pos=0, gamma_neg=4, clip=0.05)
        >>> loss = criterion(outputs, labels)
    """

    def __init__(
        self,
        gamma_pos: float = 0,
        gamma_neg: float = 4,
        clip: float = 0.05,
    ):
        super().__init__()
        self.gamma_pos = gamma_pos
        self.gamma_neg = gamma_neg
        self.clip = clip

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: ëª¨ë¸ ì¶œë ¥ logits (shape: [batch_size, num_classes])
            targets: ì •ë‹µ ë ˆì´ë¸” (shape: [batch_size])

        Returns:
            Asymmetric loss (scalar)
        """
        # One-hot encoding
        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()

        # Sigmoid í™•ë¥ 
        probs = torch.sigmoid(inputs)

        # Clip negative probabilities
        probs = torch.clamp(probs, min=self.clip)

        # Positive loss (Focal Loss)
        pos_loss = -targets_one_hot * (1 - probs) ** self.gamma_pos * torch.log(probs)

        # Negative loss (Focal Loss)
        neg_loss = -(1 - targets_one_hot) * probs ** self.gamma_neg * torch.log(1 - probs)

        # Total loss
        loss = pos_loss + neg_loss
        return loss.mean()


class LabelSmoothingCrossEntropy(nn.Module):
    """
    Label Smoothing Cross Entropy Loss

    One-hot encoding ëŒ€ì‹  smooth label ì‚¬ìš©:
    - ì •ë‹µ í´ë˜ìŠ¤: 1 - smoothing
    - ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤: smoothing / (num_classes - 1)

    ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

    Args:
        smoothing: Label smoothing ë¹„ìœ¨ (0~1, ê¸°ë³¸ê°’: 0.1)
        reduction: 'mean' ë˜ëŠ” 'sum'
        weight: í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ (Optional[Tensor])

    ì˜ˆì‹œ:
        >>> criterion = LabelSmoothingCrossEntropy(smoothing=0.1)
        >>> loss = criterion(outputs, labels)
    """

    def __init__(
        self,
        smoothing: float = 0.1,
        reduction: str = 'mean',
        weight: Optional[torch.Tensor] = None,
    ):
        super().__init__()
        self.smoothing = smoothing
        self.reduction = reduction
        self.weight = weight

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: ëª¨ë¸ ì¶œë ¥ logits (shape: [batch_size, num_classes])
            targets: ì •ë‹µ ë ˆì´ë¸” (shape: [batch_size])

        Returns:
            Label smoothing cross entropy loss (scalar)
        """
        num_classes = inputs.size(1)

        # Log softmax
        log_probs = F.log_softmax(inputs, dim=1)

        # Smooth labels
        with torch.no_grad():
            # One-hot encoding
            true_dist = torch.zeros_like(log_probs)
            true_dist.fill_(self.smoothing / (num_classes - 1))
            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)

        # Loss ê³„ì‚°
        loss = -torch.sum(true_dist * log_probs, dim=1)

        # Class weight ì ìš©
        if self.weight is not None:
            weight = self.weight[targets]
            loss = loss * weight

        # Reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss


def create_loss_from_config(cfg, device: str = 'cuda') -> nn.Module:
    """
    Hydra configë¡œë¶€í„° Loss í•¨ìˆ˜ ìƒì„±

    Args:
        cfg: Hydra config (cfg.loss ì„¹ì…˜)
        device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')

    Returns:
        Loss í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤

    ì˜ˆì‹œ:
        >>> @hydra.main(config_path="configs", config_name="config")
        >>> def main(cfg):
        >>>     criterion = create_loss_from_config(cfg)
    """
    loss_cfg = cfg.loss
    loss_type = loss_cfg.get('type', 'cross_entropy')

    print(f"ğŸ“Š Loss í•¨ìˆ˜ ìƒì„±: {loss_type}")

    if loss_type == 'focal':
        # Focal Loss
        alpha = loss_cfg.get('alpha', 0.25)
        gamma = loss_cfg.get('gamma', 2.0)
        label_smoothing = loss_cfg.get('label_smoothing', 0.0)

        # Class weights
        class_weights = loss_cfg.get('class_weights', None)
        if class_weights is not None:
            class_weights = torch.tensor(class_weights).float().to(device)

        criterion = FocalLoss(
            alpha=alpha,
            gamma=gamma,
            label_smoothing=label_smoothing,
            weight=class_weights,
        )

        print(f"   Alpha: {alpha}")
        print(f"   Gamma: {gamma}")
        if label_smoothing > 0:
            print(f"   Label Smoothing: {label_smoothing}")
        if class_weights is not None:
            print(f"   Class Weights: {class_weights.cpu().tolist()}")

    elif loss_type == 'asymmetric':
        # Asymmetric Loss
        gamma_pos = loss_cfg.get('gamma_pos', 0)
        gamma_neg = loss_cfg.get('gamma_neg', 4)
        clip = loss_cfg.get('clip', 0.05)

        criterion = AsymmetricLoss(
            gamma_pos=gamma_pos,
            gamma_neg=gamma_neg,
            clip=clip,
        )

        print(f"   Gamma Pos: {gamma_pos}")
        print(f"   Gamma Neg: {gamma_neg}")
        print(f"   Clip: {clip}")

    elif loss_type == 'label_smoothing':
        # Label Smoothing CE
        smoothing = loss_cfg.get('label_smoothing', 0.1)

        # Class weights
        class_weights = loss_cfg.get('class_weights', None)
        if class_weights is not None:
            class_weights = torch.tensor(class_weights).float().to(device)

        criterion = LabelSmoothingCrossEntropy(
            smoothing=smoothing,
            weight=class_weights,
        )

        print(f"   Smoothing: {smoothing}")
        if class_weights is not None:
            print(f"   Class Weights: {class_weights.cpu().tolist()}")

    elif loss_type == 'weighted':
        # Weighted Cross Entropy
        class_weights = loss_cfg.get('class_weights', None)
        if class_weights is not None:
            class_weights = torch.tensor(class_weights).float().to(device)

        label_smoothing = loss_cfg.get('label_smoothing', 0.0)

        criterion = nn.CrossEntropyLoss(
            weight=class_weights,
            label_smoothing=label_smoothing,
        )

        print(f"   Class Weights: {class_weights.cpu().tolist() if class_weights is not None else 'None'}")
        if label_smoothing > 0:
            print(f"   Label Smoothing: {label_smoothing}")

    else:
        # ì¼ë°˜ Cross Entropy
        label_smoothing = loss_cfg.get('label_smoothing', 0.0)

        criterion = nn.CrossEntropyLoss(
            label_smoothing=label_smoothing,
        )

        if label_smoothing > 0:
            print(f"   Label Smoothing: {label_smoothing}")

    print(f"\nğŸ’¡ íŒ: Focal LossëŠ” í´ë˜ìŠ¤ 3-7, 14 ê°™ì€ ì–´ë ¤ìš´ ìƒ˜í”Œì— ìë™ ì§‘ì¤‘!")
    print(f"   gamma=2.0 (ë³´í†µ) â†’ gamma=3.0 (ê°•í•˜ê²Œ) â†’ gamma=5.0 (ë§¤ìš° ê°•í•˜ê²Œ)")

    return criterion
