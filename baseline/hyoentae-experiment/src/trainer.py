"""
Trainer ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ Trainer í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
í•™ìŠµ, ê²€ì¦, Early Stopping, MixUp/CutMix ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from typing import Optional, Callable

from src.utils.metrics import (
    calculate_macro_f1,
    calculate_class_f1,
    accuracy_from_logits,
    MetricTracker
)
from src.utils.logger import WandBLogger
from src.utils.checkpoint import CheckpointManager
from src.utils.mixup import mixup_criterion


class Trainer:
    """
    ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ Trainer í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - Train/Validation ë£¨í”„
    - MixUp/CutMix ì§€ì›
    - WandB ë¡œê¹…
    - Early Stopping
    - Best ëª¨ë¸ ì €ì¥
    - Mixed Precision Training (AMP)

    ì˜ˆì‹œ:
        >>> trainer = Trainer(
        >>>     model=model,
        >>>     train_loader=train_loader,
        >>>     val_loader=val_loader,
        >>>     criterion=criterion,
        >>>     optimizer=optimizer,
        >>>     device='cuda',
        >>>     logger=wandb_logger,
        >>>     checkpoint_manager=ckpt_manager
        >>> )
        >>> trainer.train(num_epochs=50)
    """

    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        criterion: nn.Module,
        optimizer: torch.optim.Optimizer,
        device: str,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        logger: Optional[WandBLogger] = None,
        checkpoint_manager: Optional[CheckpointManager] = None,
        mixup_cutmix: Optional[Callable] = None,
        use_amp: bool = False,
        model_config: Optional[dict] = None,
    ):
        """
        Trainer ì´ˆê¸°í™”

        Args:
            model: PyTorch ëª¨ë¸
            train_loader: Train DataLoader
            val_loader: Validation DataLoader
            criterion: Loss í•¨ìˆ˜
            optimizer: Optimizer
            device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
            scheduler: Learning rate scheduler (ì˜µì…˜)
            logger: WandB ë¡œê±° (ì˜µì…˜)
            checkpoint_manager: ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € (ì˜µì…˜)
            mixup_cutmix: MixUp/CutMix ì¦ê°• í•¨ìˆ˜ (ì˜µì…˜)
            model_config: ëª¨ë¸ config (ì¶”ë¡  ì‹œ ì¬í˜„ìš©)
            use_amp: Mixed Precision Training ì‚¬ìš© ì—¬ë¶€
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        self.scheduler = scheduler
        self.logger = logger
        self.checkpoint_manager = checkpoint_manager
        self.mixup_cutmix = mixup_cutmix
        self.use_amp = use_amp
        self.model_config = model_config  # ì¶”ë¡  ì‹œ ì¬í˜„ìš©

        # Mixed Precision Trainingìš© GradScaler
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()
            print("âœ… Mixed Precision Training (AMP) í™œì„±í™”")

        # Metric Tracker
        self.metric_tracker = MetricTracker()

        print("âœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ")

    def train_epoch(self, epoch: int) -> dict:
        """
        1 Epoch í•™ìŠµ

        Args:
            epoch: í˜„ì¬ epoch ë²ˆí˜¸

        Returns:
            í•™ìŠµ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
            {'loss': float, 'accuracy': float, 'macro_f1': float, 'lr': float}
        """
        self.model.train()

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        # Macro F1 ê³„ì‚°ìš©
        all_predictions = []
        all_labels = []

        # Progress bar
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch} [Train]")

        for batch_idx, (images, labels) in enumerate(pbar):
            images = images.to(self.device)
            labels = labels.to(self.device)

            # ì›ë³¸ labels ì €ì¥ (Macro F1 ê³„ì‚°ìš©)
            original_labels = labels.clone()

            # MixUp/CutMix ì ìš© (ìˆìœ¼ë©´)
            if self.mixup_cutmix is not None:
                images, labels_a, labels_b, lam = self.mixup_cutmix(images, labels)

            # Forward pass
            if self.use_amp:
                # Mixed Precision
                with torch.cuda.amp.autocast():
                    outputs = self.model(images)

                    # Loss ê³„ì‚°
                    if self.mixup_cutmix is not None:
                        loss = mixup_criterion(self.criterion, outputs, labels_a, labels_b, lam)
                    else:
                        loss = self.criterion(outputs, labels)
            else:
                # ì¼ë°˜ precision
                outputs = self.model(images)

                if self.mixup_cutmix is not None:
                    loss = mixup_criterion(self.criterion, outputs, labels_a, labels_b, lam)
                else:
                    loss = self.criterion(outputs, labels)

            # Backward pass
            self.optimizer.zero_grad()

            if self.use_amp:
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                self.optimizer.step()

            # ë©”íŠ¸ë¦­ ê³„ì‚°
            total_loss += loss.item() * images.size(0)
            total_samples += images.size(0)

            # Predictionsì™€ labels ì €ì¥ (Macro F1ìš©)
            predictions = outputs.argmax(dim=1)
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(original_labels.cpu().numpy())

            # Accuracy
            if self.mixup_cutmix is None:
                correct = (predictions == labels).sum().item()
                total_correct += correct

            # Progress bar ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'loss': loss.item(),
                'avg_loss': total_loss / total_samples,
            })

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        all_predictions = np.array(all_predictions)
        all_labels = np.array(all_labels)

        # Epoch í‰ê·  ë©”íŠ¸ë¦­
        avg_loss = total_loss / total_samples
        avg_accuracy = total_correct / total_samples if self.mixup_cutmix is None else 0.0

        # Train Macro F1 ê³„ì‚°
        train_macro_f1 = calculate_macro_f1(all_predictions, all_labels)

        # Learning rate
        current_lr = self.optimizer.param_groups[0]['lr']

        metrics = {
            'train_loss': avg_loss,
            'train_accuracy': avg_accuracy,
            'train_macro_f1': train_macro_f1,
            'lr': current_lr,
        }

        return metrics

    @torch.no_grad()
    def validate_epoch(self, epoch: int) -> dict:
        """
        1 Epoch ê²€ì¦

        Args:
            epoch: í˜„ì¬ epoch ë²ˆí˜¸

        Returns:
            ê²€ì¦ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
            {'loss': float, 'accuracy': float, 'macro_f1': float}
        """
        self.model.eval()

        total_loss = 0.0
        total_samples = 0

        all_predictions = []
        all_labels = []

        # Progress bar
        pbar = tqdm(self.val_loader, desc=f"Epoch {epoch} [Val]")

        for images, labels in pbar:
            images = images.to(self.device)
            labels = labels.to(self.device)

            # Forward pass
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)

            # ë©”íŠ¸ë¦­ ëˆ„ì 
            total_loss += loss.item() * images.size(0)
            total_samples += images.size(0)

            # Prediction
            predictions = outputs.argmax(dim=1)
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # Progress bar ì—…ë°ì´íŠ¸
            pbar.set_postfix({'loss': loss.item()})

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        all_predictions = np.array(all_predictions)
        all_labels = np.array(all_labels)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_loss = total_loss / total_samples
        accuracy = (all_predictions == all_labels).mean()
        macro_f1 = calculate_macro_f1(all_predictions, all_labels)

        # í´ë˜ìŠ¤ë³„ F1 (ë¡œê¹…ìš©)
        class_f1 = calculate_class_f1(all_predictions, all_labels, num_classes=17)

        metrics = {
            'val_loss': avg_loss,
            'val_accuracy': accuracy,
            'val_macro_f1': macro_f1,
            'class_f1': class_f1,
        }

        return metrics

    def train(self, num_epochs: int, start_epoch: int = 0):
        """
        ì „ì²´ í•™ìŠµ ë£¨í”„

        Args:
            num_epochs: ì´ epoch ìˆ˜
            start_epoch: ì‹œì‘ epoch (ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ ì‹œ ì‚¬ìš©)
        """
        print(f"\nğŸš€ í•™ìŠµ ì‹œì‘: {num_epochs} epochs")
        print(f"   Device: {self.device}")
        print(f"   Train batches: {len(self.train_loader)}")
        print(f"   Val batches: {len(self.val_loader)}")
        print()

        for epoch in range(start_epoch, num_epochs):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch+1}/{num_epochs}")
            print(f"{'='*60}")

            # Train
            train_metrics = self.train_epoch(epoch + 1)

            # Validation
            val_metrics = self.validate_epoch(epoch + 1)

            # Scheduler step (ìˆìœ¼ë©´)
            if self.scheduler is not None:
                self.scheduler.step()

            # Train-Val ì°¨ì´ ê³„ì‚° (ê³¼ì í•© ëª¨ë‹ˆí„°ë§)
            loss_gap = train_metrics['train_loss'] - val_metrics['val_loss']
            f1_gap = val_metrics['val_macro_f1'] - train_metrics['train_macro_f1']

            # ë©”íŠ¸ë¦­ ì¶œë ¥
            print(f"\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
            print(f"   Train Loss: {train_metrics['train_loss']:.4f}")
            print(f"   Train Macro F1: {train_metrics['train_macro_f1']:.4f}")
            print(f"   Val Loss: {val_metrics['val_loss']:.4f}")
            print(f"   Val Accuracy: {val_metrics['val_accuracy']:.4f}")
            print(f"   Val Macro F1: {val_metrics['val_macro_f1']:.4f}")
            print(f"   Learning Rate: {train_metrics['lr']:.6f}")

            # Train-Val ì°¨ì´ ì¶œë ¥ (ê³¼ì í•© ê°ì§€)
            print(f"\nğŸ” Train-Val ì°¨ì´ (ê³¼ì í•© ëª¨ë‹ˆí„°ë§):")
            print(f"   Loss Gap: {loss_gap:+.4f} (Train - Val)")
            print(f"   F1 Gap: {f1_gap:+.4f} (Val - Train)")

            # ê³¼ì í•© ê²½ê³ 
            if f1_gap < -0.05:  # Train F1ì´ Val F1ë³´ë‹¤ 5%p ì´ìƒ ë†’ìœ¼ë©´
                print(f"   âš ï¸  ê³¼ì í•© ì£¼ì˜! Train F1ì´ Val F1ë³´ë‹¤ {abs(f1_gap):.1%} ë†’ìŠµë‹ˆë‹¤")
            elif f1_gap > 0.05:  # Val F1ì´ Train F1ë³´ë‹¤ 5%p ì´ìƒ ë†’ìœ¼ë©´
                print(f"   âš ï¸  ê³¼ì†Œì í•© ì£¼ì˜! Val F1ì´ Train F1ë³´ë‹¤ {f1_gap:.1%} ë†’ìŠµë‹ˆë‹¤")
            else:
                print(f"   âœ… ì ì ˆí•œ í•™ìŠµ ìƒíƒœ (ì°¨ì´: {abs(f1_gap):.1%})")

            # WandB ë¡œê¹…
            if self.logger is not None:
                # ê¸°ë³¸ ë©”íŠ¸ë¦­
                log_dict = {**train_metrics, **val_metrics}
                # í´ë˜ìŠ¤ë³„ F1 ì œì™¸ (ë„ˆë¬´ ë§ì•„ì„œ)
                log_dict.pop('class_f1', None)

                # Train-Val ì°¨ì´ ì¶”ê°€
                log_dict['train_val_loss_gap'] = loss_gap
                log_dict['train_val_f1_gap'] = f1_gap

                self.logger.log(log_dict, step=epoch + 1)

                # í´ë˜ìŠ¤ë³„ F1 (ë³„ë„ ë¡œê¹…)
                self.logger.log_class_metrics(val_metrics['class_f1'], step=epoch + 1)

            # Metric Tracker ì—…ë°ì´íŠ¸
            self.metric_tracker.update(val_metrics['val_macro_f1'], epoch=epoch + 1)

            # Checkpoint ì €ì¥
            if self.checkpoint_manager is not None:
                extra_info = {
                    'val_loss': val_metrics['val_loss'],
                    'val_accuracy': val_metrics['val_accuracy'],
                }

                # ëª¨ë¸ config ì¶”ê°€ (ì¶”ë¡  ì‹œ ìë™ ëª¨ë¸ ë¡œë“œìš©)
                if self.model_config is not None:
                    extra_info['model_config'] = self.model_config

                is_best = self.checkpoint_manager.save_checkpoint(
                    model=self.model,
                    optimizer=self.optimizer,
                    epoch=epoch + 1,
                    metric_value=val_metrics['val_macro_f1'],
                    train_metric_value=train_metrics['train_macro_f1'],  # Train ë©”íŠ¸ë¦­ ì¶”ê°€
                    scheduler=self.scheduler,
                    extra_info=extra_info
                )

                # Early Stopping ì²´í¬
                if self.checkpoint_manager.should_stop():
                    print(f"\nâš ï¸  Early Stopping ë°œë™! (Patience={self.checkpoint_manager.patience})")
                    print(f"   Best Macro F1: {self.checkpoint_manager.get_best_metric():.4f}")
                    print(f"   Best Epoch: {self.checkpoint_manager.get_best_epoch()}")
                    break

        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        if self.checkpoint_manager is not None:
            print(f"   Best Macro F1: {self.checkpoint_manager.get_best_metric():.4f}")
            print(f"   Best Epoch: {self.checkpoint_manager.get_best_epoch()}")


def create_optimizer(model: nn.Module, cfg) -> torch.optim.Optimizer:
    """
    Configë¡œë¶€í„° Optimizer ìƒì„±

    Args:
        model: PyTorch ëª¨ë¸
        cfg: Hydra config (cfg.train.optimizer ì„¹ì…˜)

    Returns:
        Optimizer

    ì˜ˆì‹œ:
        >>> optimizer = create_optimizer(model, cfg)
    """
    optimizer_cfg = cfg.train.optimizer

    optimizer_name = optimizer_cfg.get('name', 'AdamW').lower()
    lr = optimizer_cfg.get('lr', 0.001)
    weight_decay = optimizer_cfg.get('weight_decay', 0.01)

    if optimizer_name == 'adamw':
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
        )
    elif optimizer_name == 'adam':
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
        )
    elif optimizer_name == 'sgd':
        momentum = optimizer_cfg.get('momentum', 0.9)
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=lr,
            momentum=momentum,
            weight_decay=weight_decay,
        )
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    print(f"âœ… Optimizer ìƒì„±: {optimizer_name}")
    print(f"   Learning Rate: {lr}")
    print(f"   Weight Decay: {weight_decay}")

    return optimizer


def create_scheduler(optimizer: torch.optim.Optimizer, cfg, num_epochs: int):
    """
    Configë¡œë¶€í„° Learning Rate Scheduler ìƒì„±

    Args:
        optimizer: Optimizer
        cfg: Hydra config (cfg.train.scheduler ì„¹ì…˜)
        num_epochs: ì´ epoch ìˆ˜

    Returns:
        Scheduler ë˜ëŠ” None

    ì˜ˆì‹œ:
        >>> scheduler = create_scheduler(optimizer, cfg, num_epochs=50)
    """
    scheduler_cfg = cfg.train.get('scheduler', None)

    if scheduler_cfg is None:
        return None

    scheduler_name = scheduler_cfg.get('name', 'cosine').lower()

    if scheduler_name == 'cosine':
        # Cosine Annealing with Warmup
        warmup_epochs = scheduler_cfg.get('warmup_epochs', 5)
        min_lr = scheduler_cfg.get('min_lr', 1e-5)

        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=num_epochs - warmup_epochs,
            eta_min=min_lr,
        )

        print(f"âœ… Scheduler ìƒì„±: CosineAnnealing")
        print(f"   Warmup Epochs: {warmup_epochs}")
        print(f"   Min LR: {min_lr}")

    elif scheduler_name == 'step':
        step_size = scheduler_cfg.get('step_size', 10)
        gamma = scheduler_cfg.get('gamma', 0.1)

        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=step_size,
            gamma=gamma,
        )

        print(f"âœ… Scheduler ìƒì„±: StepLR")
        print(f"   Step Size: {step_size}")
        print(f"   Gamma: {gamma}")

    else:
        print(f"âš ï¸  Unknown scheduler: {scheduler_name}, ìŠ¤ì¼€ì¤„ëŸ¬ ì—†ì´ ì§„í–‰")
        return None

    return scheduler
