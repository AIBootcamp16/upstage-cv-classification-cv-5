"""
Train Script

Hydraì™€ WandBë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ
    python train.py model=efficientnet_b0

    # ê°•ë ¥í•œ ì¦ê°• ì‚¬ìš©
    python train.py model=efficientnet_b0 augmentation=strong

    # 4ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í—˜ (Hydra multi-run)
    python train.py -m model=efficientnet_b0,efficientnet_b3,resnet50,convnext_tiny

    # Augmentation ë¹„êµ ì‹¤í—˜
    python train.py -m augmentation=default,strong
"""

import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf
import torch
import torch.nn as nn
import os
import random
import numpy as np
from pathlib import Path

from src.models.classifier import create_model_from_config
from src.models.losses import create_loss_from_config
from src.data.dataset import create_train_val_datasets, create_dataloaders, ClassConditionalAugraphyDataset
from src.data.transforms import create_transforms_from_config, create_augraphy_pipeline
from src.trainer import Trainer, create_optimizer, create_scheduler
from src.utils.logger import create_logger_from_config
from src.utils.checkpoint import CheckpointManager
from src.utils.mixup import create_mixup_cutmix_from_config
from src.utils.kfold import create_kfold_splits, print_kfold_info, get_fold_save_dir, save_kfold_summary


def set_seed(seed: int = 42):
    """
    ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ê³ ì •

    Args:
        seed: ëœë¤ ì‹œë“œ ê°’
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # CuDNN ê²°ì •ë¡ ì  ë™ì‘ (ì†ë„ ì•½ê°„ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"âœ… ëœë¤ ì‹œë“œ ê³ ì •: {seed}")


def get_device(cfg) -> str:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ í™•ì¸

    CUDA ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU, ì•„ë‹ˆë©´ CPU ì‚¬ìš©

    Args:
        cfg: Hydra config

    Returns:
        ë””ë°”ì´ìŠ¤ ë¬¸ìì—´ ('cuda' ë˜ëŠ” 'cpu')
    """
    device_from_cfg = cfg.get('device', 'cuda')

    if device_from_cfg == 'cuda' and torch.cuda.is_available():
        device = 'cuda'
        print(f"âœ… GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
    else:
        device = 'cpu'
        if device_from_cfg == 'cuda':
            print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ fallbackí•©ë‹ˆë‹¤.")
        print("âœ… CPU ì‚¬ìš©")

    return device


@hydra.main(config_path="configs", config_name="config", version_base=None)
def main(cfg: DictConfig):
    """
    ë©”ì¸ í•™ìŠµ í•¨ìˆ˜

    Hydraê°€ ìë™ìœ¼ë¡œ configë¥¼ ë¡œë“œí•˜ê³  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼
    outputs/YYYY-MM-DD/HH-MM-SS/ ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

    Args:
        cfg: Hydra config
    """
    # Config ì¶œë ¥
    print("\n" + "="*60)
    print("âš™ï¸  Config")
    print("="*60)
    print(OmegaConf.to_yaml(cfg))
    print("="*60 + "\n")

    # ëœë¤ ì‹œë“œ ê³ ì •
    seed = cfg.get('seed', 42)
    set_seed(seed)

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = get_device(cfg)

    # ë°ì´í„° ê²½ë¡œ
    data_dir = Path(cfg.data.data_dir)
    train_csv = data_dir / cfg.data.train_csv
    train_img_dir = data_dir / cfg.data.train_dir

    print(f"\nğŸ“‚ ë°ì´í„° ê²½ë¡œ:")
    print(f"   CSV: {train_csv}")
    print(f"   Images: {train_img_dir}\n")

    # K-Fold ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_kfold = cfg.train.get('k_fold', {}).get('enabled', False)

    if use_kfold:
        # ========== K-Fold Cross Validation ëª¨ë“œ ==========
        print(f"\nğŸ”€ K-Fold Cross Validation ëª¨ë“œ í™œì„±í™”")

        n_splits = cfg.train.k_fold.get('n_splits', 5)
        shuffle_kfold = cfg.train.k_fold.get('shuffle', True)

        # K-Fold ë¶„í•  ìƒì„±
        kfold_splits = create_kfold_splits(
            train_csv=str(train_csv),
            n_splits=n_splits,
            shuffle=shuffle_kfold,
            random_state=seed
        )

        # K-Fold ì •ë³´ ì¶œë ¥
        print_kfold_info(kfold_splits, str(train_csv))

        # Hydraì˜ ì‹¤ì œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        hydra_cfg = HydraConfig.get()
        hydra_output_dir = hydra_cfg.runtime.output_dir

        # ê° Foldë³„ë¡œ í•™ìŠµ
        fold_results = []

        for fold_idx, (train_idx, val_idx) in enumerate(kfold_splits):
            print(f"\n{'='*60}")
            print(f"ğŸ”€ Fold {fold_idx+1}/{n_splits} í•™ìŠµ ì‹œì‘")
            print(f"{'='*60}")

            # Foldë³„ ì €ì¥ ë””ë ‰í† ë¦¬
            fold_save_dir = get_fold_save_dir(hydra_output_dir, fold_idx)
            fold_save_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ’¾ Fold {fold_idx} ì €ì¥ ë””ë ‰í† ë¦¬: {fold_save_dir}")

            # Transforms ìƒì„±
            train_transform = create_transforms_from_config(cfg, mode='train')
            val_transform = create_transforms_from_config(cfg, mode='valid')

            # Dataset ìƒì„± (ì„ì‹œ CSV ìƒì„±)
            from src.data.dataset import DocumentDataset, ClassConditionalAugraphyDataset
            from src.data.transforms import create_augraphy_pipeline
            import pandas as pd
            import tempfile

            df = pd.read_csv(str(train_csv))

            # ì„ì‹œ CSV íŒŒì¼ ìƒì„± (Foldë³„)
            train_fold_df = df.iloc[train_idx]
            val_fold_df = df.iloc[val_idx]

            # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_train.csv') as train_csv_file:
                train_fold_df.to_csv(train_csv_file.name, index=False)
                train_csv_path = train_csv_file.name

            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_val.csv') as val_csv_file:
                val_fold_df.to_csv(val_csv_file.name, index=False)
                val_csv_path = val_csv_file.name

            # Augraphy ì„¤ì • í™•ì¸
            use_conditional_augraphy = False
            augraphy_pipeline = None
            target_classes = None
            augraphy_probability = 0.7

            augraphy_cfg = cfg.get('augmentation', {}).get('augraphy', {})
            if augraphy_cfg.get('enabled', False):
                try:
                    import augraphy
                    AUGRAPHY_AVAILABLE = True
                except ImportError:
                    AUGRAPHY_AVAILABLE = False

                if AUGRAPHY_AVAILABLE:
                    target_classes = augraphy_cfg.get('target_classes', None)
                    if target_classes is not None:
                        # target_classesê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ClassConditionalAugraphyDataset ì‚¬ìš©
                        use_conditional_augraphy = True
                        strength = augraphy_cfg.get('strength', 'medium')
                        augraphy_probability = augraphy_cfg.get('probability', 0.7)
                        augraphy_pipeline = create_augraphy_pipeline(strength=strength)
                        print(f"   ğŸ¯ ClassConditionalAugraphyDataset ì‚¬ìš© (target_classes={target_classes})")

            # Dataset ìƒì„±
            if use_conditional_augraphy:
                # ClassConditionalAugraphyDataset ì‚¬ìš©
                train_dataset_fold = ClassConditionalAugraphyDataset(
                    csv_path=train_csv_path,
                    img_dir=str(train_img_dir),
                    base_transform=train_transform,
                    augraphy_pipeline=augraphy_pipeline,
                    target_classes=target_classes,
                    augraphy_probability=augraphy_probability,
                    is_test=False
                )
            else:
                # ì¼ë°˜ DocumentDataset ì‚¬ìš©
                train_dataset_fold = DocumentDataset(
                    csv_path=train_csv_path,
                    img_dir=str(train_img_dir),
                    transform=train_transform
                )

            val_dataset_fold = DocumentDataset(
                csv_path=val_csv_path,
                img_dir=str(train_img_dir),
                transform=val_transform
            )

            # DataLoader ìƒì„±
            train_loader, val_loader = create_dataloaders(
                train_dataset=train_dataset_fold,
                val_dataset=val_dataset_fold,
                batch_size=cfg.train.batch_size,
                num_workers=cfg.data.get('num_workers', 4),
            )

            # ëª¨ë¸ ìƒì„± (ê° foldë§ˆë‹¤ ìƒˆë¡œ ìƒì„±)
            model = create_model_from_config(cfg, num_classes=cfg.data.num_classes)

            # Loss í•¨ìˆ˜ (Focal Loss, Label Smoothing ë“±)
            # label_smoothingì„ cfg.trainì—ì„œ cfg.lossë¡œ ì „ë‹¬
            if not hasattr(cfg, 'loss'):
                cfg.loss = {}
            if 'label_smoothing' not in cfg.loss and 'label_smoothing' in cfg.train:
                OmegaConf.set_struct(cfg, False)
                cfg.loss.label_smoothing = cfg.train.label_smoothing
                OmegaConf.set_struct(cfg, True)
            criterion = create_loss_from_config(cfg, device=device)

            # Optimizer
            optimizer = create_optimizer(model, cfg)

            # Scheduler
            scheduler = create_scheduler(optimizer, cfg, num_epochs=cfg.train.epochs)

            # WandB Logger (Foldë³„)
            model_name = cfg.model.name
            aug_type = cfg.get('augmentation', {}).get('name', 'default')

            # K-Foldìš© ì‹¤í—˜ ì´ë¦„ (fold ë²ˆí˜¸ ì¶”ê°€)
            if cfg.wandb.get('name') is None:
                fold_experiment_name = f"{model_name}_{aug_type}_fold{fold_idx}"
            else:
                fold_experiment_name = f"{cfg.wandb.name}_fold{fold_idx}"

            # Config ë³µì‚¬í•´ì„œ ì´ë¦„ë§Œ ë³€ê²½
            fold_cfg = OmegaConf.to_container(cfg, resolve=True)
            fold_cfg = OmegaConf.create(fold_cfg)
            OmegaConf.set_struct(fold_cfg, False)
            fold_cfg.wandb.name = fold_experiment_name
            fold_cfg.wandb.tags = [model_name, aug_type, f"fold_{fold_idx}"]
            OmegaConf.set_struct(fold_cfg, True)

            logger = create_logger_from_config(fold_cfg)

            # Checkpoint Manager (Foldë³„)
            use_gen_score = cfg.train.get('use_generalization_score', True)
            overfitting_penalty = cfg.train.get('overfitting_penalty', 0.3)

            checkpoint_manager = CheckpointManager(
                save_dir=str(fold_save_dir),
                metric_name="macro_f1",
                mode="max",
                patience=cfg.train.early_stopping.patience,
                verbose=True,
                use_generalization_score=use_gen_score,
                overfitting_penalty=overfitting_penalty,
            )

            # MixUp/CutMix
            mixup_cutmix = create_mixup_cutmix_from_config(cfg)
            if fold_idx == 0 and mixup_cutmix is not None:
                # Fold 0ì—ì„œë§Œ ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
                mixup_alpha = cfg.train.get('mixup_alpha', 0.0)
                cutmix_alpha = cfg.train.get('cutmix_alpha', 0.0)
                print(f"âœ… MixUp/CutMix í™œì„±í™” (mixup_alpha={mixup_alpha}, cutmix_alpha={cutmix_alpha})")

            # Mixed Precision Training
            use_amp = cfg.train.get('use_amp', False)

            # Trainer ìƒì„±
            trainer = Trainer(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                criterion=criterion,
                optimizer=optimizer,
                device=device,
                scheduler=scheduler,
                logger=logger,
                checkpoint_manager=checkpoint_manager,
                mixup_cutmix=mixup_cutmix,
                use_amp=use_amp,
                model_config=dict(cfg.model),
            )

            # í•™ìŠµ ì‹œì‘
            trainer.train(num_epochs=cfg.train.epochs)

            # WandB ì¢…ë£Œ
            if logger is not None:
                logger.finish()

            # Fold ê²°ê³¼ ì €ì¥
            fold_result = {
                'fold': fold_idx,
                'best_epoch': checkpoint_manager.get_best_epoch(),
                'val_f1': checkpoint_manager.get_best_metric(),
                'train_f1': checkpoint_manager.get_best_train_metric(),
            }
            fold_results.append(fold_result)

            print(f"\nâœ… Fold {fold_idx} í•™ìŠµ ì™„ë£Œ!")
            print(f"   Best Epoch: {fold_result['best_epoch']}")
            print(f"   Val Macro F1: {fold_result['val_f1']:.4f}")
            if fold_result['train_f1'] is not None:
                print(f"   Train Macro F1: {fold_result['train_f1']:.4f}")

        # K-Fold ì „ì²´ ê²°ê³¼ ìš”ì•½
        save_kfold_summary(hydra_output_dir, fold_results)

        return  # K-Fold ëª¨ë“œëŠ” ì—¬ê¸°ì„œ ì¢…ë£Œ

    # ========== ê¸°ì¡´ ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ ëª¨ë“œ (K-Fold ë¹„í™œì„±í™”) ==========
    # Transforms ìƒì„±
    train_transform = create_transforms_from_config(cfg, mode='train')
    val_transform = create_transforms_from_config(cfg, mode='valid')

    # Dataset ìƒì„± (cfg ì „ë‹¬í•˜ì—¬ Augraphy target_classes ì§€ì›)
    train_dataset, val_dataset = create_train_val_datasets(
        csv_path=str(train_csv),
        img_dir=str(train_img_dir),
        train_transform=train_transform,
        val_transform=val_transform,
        val_split=cfg.data.val_split,
        random_state=seed,
        cfg=cfg,  # Augraphy target_classes ì§€ì›
    )

    # DataLoader ìƒì„±
    train_loader, val_loader = create_dataloaders(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        batch_size=cfg.train.batch_size,
        num_workers=cfg.data.get('num_workers', 4),
    )

    # ëª¨ë¸ ìƒì„±
    model = create_model_from_config(cfg, num_classes=cfg.data.num_classes)

    # Loss í•¨ìˆ˜ (Focal Loss, Label Smoothing ë“±)
    # label_smoothingì„ cfg.trainì—ì„œ cfg.lossë¡œ ì „ë‹¬
    if not hasattr(cfg, 'loss'):
        cfg.loss = {}
    if 'label_smoothing' not in cfg.loss and 'label_smoothing' in cfg.train:
        OmegaConf.set_struct(cfg, False)
        cfg.loss.label_smoothing = cfg.train.label_smoothing
        OmegaConf.set_struct(cfg, True)
    criterion = create_loss_from_config(cfg, device=device)

    # Optimizer
    optimizer = create_optimizer(model, cfg)

    # Scheduler
    scheduler = create_scheduler(optimizer, cfg, num_epochs=cfg.train.epochs)

    # WandB Logger
    # ì‹¤í—˜ ì´ë¦„ ìë™ ìƒì„±: ëª¨ë¸ëª…_ì¦ê°•íƒ€ì…_vë²„ì „ (ì˜ˆ: efficientnet_b0_default_v1)
    model_name = cfg.model.name
    aug_type = cfg.get('augmentation', {}).get('name', 'default')

    # Configì˜ wandb.nameì´ Noneì´ë©´ ìë™ ìƒì„±
    if cfg.wandb.get('name') is None:
        # WandB APIë¡œ ê¸°ì¡´ run ê°œìˆ˜ í™•ì¸í•´ì„œ ë²„ì „ ë„˜ë²„ë§
        import wandb as wandb_module

        # ì„ì‹œë¡œ APIë§Œ ì´ˆê¸°í™” (ì‹¤ì œ runì€ ìƒì„± ì•ˆ í•¨)
        api = wandb_module.Api()
        entity = cfg.wandb.get('entity', None)
        if entity is None:
            entity = api.default_entity
        project_path = f"{entity}/{cfg.wandb.project}"

        try:
            # ê°™ì€ ì´ë¦„ íŒ¨í„´ì˜ run ê°œìˆ˜ ì„¸ê¸°
            base_name = f"{model_name}_{aug_type}"
            runs = api.runs(project_path)

            # ê°™ì€ base_nameìœ¼ë¡œ ì‹œì‘í•˜ëŠ” run ê°œìˆ˜ ì„¸ê¸°
            version_count = 0
            for run in runs:
                if run.name and run.name.startswith(base_name):
                    version_count += 1

            # ë²„ì „ ë²ˆí˜¸ (ê¸°ì¡´ ê°œìˆ˜ + 1)
            version = version_count + 1
            auto_experiment_name = f"{base_name}_v{version}"

        except Exception as e:
            # WandB API ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ (í”„ë¡œì íŠ¸ ì—†ìŒ ë“±) ê¸°ë³¸ê°’ v1 ì‚¬ìš©
            print(f"âš ï¸  WandB API ì ‘ê·¼ ì‹¤íŒ¨ (í”„ë¡œì íŠ¸ ì—†ê±°ë‚˜ ì²˜ìŒ ì‹¤í–‰): {e}")
            auto_experiment_name = f"{model_name}_{aug_type}_v1"

        OmegaConf.set_struct(cfg, False)  # êµ¬ì¡° ìˆ˜ì • í—ˆìš©
        cfg.wandb.name = auto_experiment_name
        # íƒœê·¸ë„ ìë™ ì—…ë°ì´íŠ¸
        cfg.wandb.tags = [model_name, aug_type]
        OmegaConf.set_struct(cfg, True)
        print(f"âœ… WandB ì‹¤í—˜ ì´ë¦„ ìë™ ì„¤ì •: {auto_experiment_name}")
        print(f"   íƒœê·¸: {[model_name, aug_type]}")
    else:
        # ì‚¬ìš©ìê°€ ì§ì ‘ ì§€ì •í•œ ì´ë¦„ ì‚¬ìš©
        print(f"âœ… WandB ì‹¤í—˜ ì´ë¦„ (ì‚¬ìš©ì ì§€ì •): {cfg.wandb.name}")
        # íƒœê·¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìë™ ì„¤ì •
        if not cfg.wandb.tags or len(cfg.wandb.tags) == 0:
            OmegaConf.set_struct(cfg, False)
            cfg.wandb.tags = [model_name, aug_type]
            OmegaConf.set_struct(cfg, True)
            print(f"   íƒœê·¸ ìë™ ì„¤ì •: {[model_name, aug_type]}")

    logger = create_logger_from_config(cfg)

    # Checkpoint Manager
    # Hydraì˜ ì‹¤ì œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    hydra_cfg = HydraConfig.get()
    hydra_output_dir = hydra_cfg.runtime.output_dir
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬: {hydra_output_dir}")

    # Generalization Score ì„¤ì • (configì—ì„œ ì½ê¸°, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    use_gen_score = cfg.train.get('use_generalization_score', True)
    overfitting_penalty = cfg.train.get('overfitting_penalty', 0.3)

    checkpoint_manager = CheckpointManager(
        save_dir=hydra_output_dir,
        metric_name="macro_f1",
        mode="max",
        patience=cfg.train.early_stopping.patience,
        verbose=True,
        use_generalization_score=use_gen_score,
        overfitting_penalty=overfitting_penalty,
    )

    if use_gen_score:
        print(f"ğŸ” Generalization Score í™œì„±í™” (ê³¼ì í•© í˜ë„í‹°: {overfitting_penalty})")
    else:
        print(f"ğŸ“Š ê¸°ë³¸ ëª¨ë“œ: Val Macro F1ë§Œ ì‚¬ìš©")

    # MixUp/CutMix (ìˆìœ¼ë©´)
    mixup_cutmix = create_mixup_cutmix_from_config(cfg)
    if mixup_cutmix is not None:
        mixup_alpha = cfg.train.get('mixup_alpha', 0.0)
        cutmix_alpha = cfg.train.get('cutmix_alpha', 0.0)
        print(f"âœ… MixUp/CutMix í™œì„±í™” (mixup_alpha={mixup_alpha}, cutmix_alpha={cutmix_alpha})")

    # Mixed Precision Training
    use_amp = cfg.train.get('use_amp', False)

    # Trainer ìƒì„±
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        device=device,
        scheduler=scheduler,
        logger=logger,
        checkpoint_manager=checkpoint_manager,
        mixup_cutmix=mixup_cutmix,
        use_amp=use_amp,
        model_config=dict(cfg.model),  # ëª¨ë¸ config ì „ë‹¬ (ì¶”ë¡  ì‹œ ì¬í˜„ìš©)
    )

    # í•™ìŠµ ì‹œì‘
    trainer.train(num_epochs=cfg.train.epochs)

    # WandB ì¢…ë£Œ
    if logger is not None:
        logger.finish()

    print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   Best Epoch: {checkpoint_manager.get_best_epoch()}")

    # Best epochì˜ Train & Val Macro F1 ì¶œë ¥
    best_train_f1 = checkpoint_manager.get_best_train_metric()
    best_val_f1 = checkpoint_manager.get_best_metric()

    if best_train_f1 is not None:
        print(f"   Best Train Macro F1: {best_train_f1:.4f}")
        print(f"   Best Val Macro F1: {best_val_f1:.4f}")
        gap = best_train_f1 - best_val_f1
        print(f"   Train-Val Gap: {gap:+.4f} ({abs(gap)*100:.1f}%)")
    else:
        print(f"   Best Val Macro F1: {best_val_f1:.4f}")

    print(f"   Checkpoint: {hydra_output_dir}/best.pth")


if __name__ == "__main__":
    main()
