"""
Evaluation Script

Validation setì—ì„œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

ì‚¬ìš©ë²•:
    # Validation set í‰ê°€
    python evaluate.py checkpoint=outputs/2025-11-02/12-00-00/best.pth

    # Config ì˜¤ë²„ë¼ì´ë“œ
    python evaluate.py checkpoint=best.pth model=efficientnet_b0
"""

import hydra
from omegaconf import DictConfig, OmegaConf
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

from src.models.classifier import create_model_from_config
from src.data.dataset import create_train_val_datasets, create_dataloaders
from src.data.transforms import create_transforms_from_config
from src.utils.checkpoint import load_model_for_inference, find_latest_checkpoint
from src.utils.metrics import (
    calculate_macro_f1,
    calculate_class_f1,
    get_classification_report,
    get_confusion_matrix
)


@torch.no_grad()
def evaluate_model(
    model: torch.nn.Module,
    val_loader,
    device: str,
    num_classes: int = 17
) -> dict:
    """
    ëª¨ë¸ í‰ê°€

    Args:
        model: PyTorch ëª¨ë¸
        val_loader: Validation DataLoader
        device: ë””ë°”ì´ìŠ¤
        num_classes: í´ë˜ìŠ¤ ìˆ˜

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    model.eval()

    all_predictions = []
    all_labels = []

    for images, labels in tqdm(val_loader, desc="Evaluating"):
        images = images.to(device)
        labels = labels.to(device)

        # ì˜ˆì¸¡
        outputs = model(images)
        predictions = outputs.argmax(dim=1)

        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    accuracy = (all_predictions == all_labels).mean()
    macro_f1 = calculate_macro_f1(all_predictions, all_labels)
    class_f1 = calculate_class_f1(all_predictions, all_labels, num_classes=num_classes)

    # Classification Report
    classification_rep = get_classification_report(
        all_predictions, all_labels, num_classes=num_classes
    )

    # Confusion Matrix
    confusion_mat = get_confusion_matrix(
        all_predictions, all_labels, num_classes=num_classes
    )

    results = {
        'accuracy': accuracy,
        'macro_f1': macro_f1,
        'class_f1': class_f1,
        'classification_report': classification_rep,
        'confusion_matrix': confusion_mat,
        'predictions': all_predictions,
        'labels': all_labels,
    }

    return results


def plot_confusion_matrix(
    confusion_matrix: np.ndarray,
    save_path: str = "confusion_matrix.png",
    num_classes: int = 17
):
    """
    Confusion Matrix ì‹œê°í™”

    Args:
        confusion_matrix: Confusion matrix ë°°ì—´
        save_path: ì €ì¥ ê²½ë¡œ
        num_classes: í´ë˜ìŠ¤ ìˆ˜
    """
    plt.figure(figsize=(14, 12))

    # í´ë˜ìŠ¤ ì´ë¦„
    class_names = [f'target_{i}' for i in range(num_classes)]

    # Heatmap
    sns.heatmap(
        confusion_matrix,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names,
        cbar_kws={'label': 'Count'}
    )

    plt.title('Confusion Matrix', fontsize=16, pad=20)
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()

    # ì €ì¥
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Confusion Matrix saved: {save_path}")

    plt.close()


def plot_class_f1_scores(
    class_f1: dict,
    save_path: str = "class_f1_scores.png"
):
    """
    í´ë˜ìŠ¤ë³„ F1 Score ë§‰ëŒ€ ê·¸ë˜í”„

    Args:
        class_f1: í´ë˜ìŠ¤ë³„ F1 ë”•ì…”ë„ˆë¦¬
        save_path: ì €ì¥ ê²½ë¡œ
    """
    plt.figure(figsize=(12, 6))

    # ë°ì´í„° ì¤€ë¹„
    classes = list(class_f1.keys())
    f1_scores = list(class_f1.values())

    # ìƒ‰ìƒ (F1ì´ ë‚®ì€ í´ë˜ìŠ¤ëŠ” ë¹¨ê°„ìƒ‰)
    colors = ['red' if f1 < 0.7 else 'green' if f1 > 0.9 else 'orange' for f1 in f1_scores]

    # ë§‰ëŒ€ ê·¸ë˜í”„
    bars = plt.bar(classes, f1_scores, color=colors, alpha=0.7, edgecolor='black')

    # ê°’ í‘œì‹œ
    for bar in bars:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f'{height:.3f}',
            ha='center',
            va='bottom',
            fontsize=9
        )

    plt.title('F1 Score by Class', fontsize=16, pad=20)
    plt.xlabel('Class', fontsize=12)
    plt.ylabel('F1 Score', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1.05)
    plt.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, label='F1=0.8')
    plt.legend()
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()

    # ì €ì¥
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… F1 Score by Class plot saved: {save_path}")

    plt.close()


@hydra.main(config_path="configs", config_name="config", version_base=None)
def main(cfg: DictConfig):
    """
    ë©”ì¸ í‰ê°€ í•¨ìˆ˜

    Args:
        cfg: Hydra config
    """
    # Config ì¶œë ¥
    print("\n" + "="*60)
    print("âš™ï¸  Evaluation Config")
    print("="*60)
    print(OmegaConf.to_yaml(cfg))
    print("="*60 + "\n")

    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    checkpoint_path = cfg.get('checkpoint', None)

    if checkpoint_path is None:
        # ìë™ìœ¼ë¡œ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        print("ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•ŠìŒ â†’ ìµœì‹  ì‹¤í—˜ ìë™ ê²€ìƒ‰")
        try:
            checkpoint_path = find_latest_checkpoint(output_dir="outputs")
        except FileNotFoundError as e:
            print(str(e))
            raise
    else:
        print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")

    # ë””ë°”ì´ìŠ¤
    if cfg.get('device', 'cuda') == 'cuda' and torch.cuda.is_available():
        device = 'cuda'
        print(f"âœ… GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
    else:
        device = 'cpu'
        print("âœ… CPU ì‚¬ìš©")

    # ë°ì´í„° ê²½ë¡œ
    data_dir = Path(cfg.data.data_dir)
    train_csv = data_dir / cfg.data.train_csv
    train_img_dir = data_dir / cfg.data.train_dir

    print(f"\nğŸ“‚ ë°ì´í„° ê²½ë¡œ:")
    print(f"   CSV: {train_csv}")
    print(f"   Images: {train_img_dir}\n")

    # Transforms
    train_transform = create_transforms_from_config(cfg, mode='train')
    val_transform = create_transforms_from_config(cfg, mode='valid')

    # Dataset (Validationë§Œ ì‚¬ìš©)
    _, val_dataset = create_train_val_datasets(
        csv_path=str(train_csv),
        img_dir=str(train_img_dir),
        train_transform=train_transform,
        val_transform=val_transform,
        val_split=cfg.data.val_split,
        random_state=cfg.get('seed', 42),
    )

    # DataLoader
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=cfg.train.get('batch_size', 32),
        shuffle=False,
        num_workers=cfg.data.get('num_workers', 4),
        pin_memory=True,
    )

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ì„¤ì • ì½ê¸°
    print(f"\nğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ model config ë³µì›
    if 'model_config' in checkpoint:
        model_cfg = checkpoint['model_config']
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ì„¤ì • ë³µì›: {model_cfg['architecture']}")

        from src.models.classifier import DocumentClassifier

        model = DocumentClassifier(
            model_name=model_cfg['architecture'],
            num_classes=model_cfg['num_classes'],
            pretrained=False,
            dropout=model_cfg.get('dropout', 0.3)
        )

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(device)
        model.eval()

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_cfg['architecture']}")

    elif 'config' in checkpoint:
        model_cfg = checkpoint['config']['model']
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ì„¤ì • ë³µì›: {model_cfg['architecture']}")

        from src.models.classifier import DocumentClassifier

        model = DocumentClassifier(
            model_name=model_cfg['architecture'],
            num_classes=model_cfg['num_classes'],
            pretrained=False,
            dropout=model_cfg.get('dropout', 0.3)
        )

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(device)
        model.eval()

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_cfg['architecture']}")

    else:
        # ì²´í¬í¬ì¸íŠ¸ì— configê°€ ì—†ìœ¼ë©´ í˜„ì¬ config ì‚¬ìš©
        print("âš ï¸  ì²´í¬í¬ì¸íŠ¸ì— ëª¨ë¸ ì„¤ì •ì´ ì—†ì–´ í˜„ì¬ config ì‚¬ìš©")
        model = create_model_from_config(cfg, num_classes=cfg.data.num_classes)
        model = load_model_for_inference(model, checkpoint_path, device)

    # í‰ê°€
    print("\nğŸ” ëª¨ë¸ í‰ê°€ ì‹œì‘...\n")
    results = evaluate_model(
        model=model,
        val_loader=val_loader,
        device=device,
        num_classes=cfg.data.num_classes
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼")
    print("="*60)
    print(f"Accuracy: {results['accuracy']:.4f}")
    print(f"Macro F1 Score: {results['macro_f1']:.4f}")
    print("="*60)

    print("\nğŸ“‹ í´ë˜ìŠ¤ë³„ F1 Score:")
    print("-"*40)
    for class_name, f1_score in results['class_f1'].items():
        status = "âœ…" if f1_score > 0.8 else "âš ï¸" if f1_score > 0.6 else "âŒ"
        print(f"  {status} {class_name}: {f1_score:.4f}")

    print("\nğŸ“ˆ Classification Report:")
    print(results['classification_report'])

    # Confusion Matrix ì‹œê°í™”
    plot_confusion_matrix(
        results['confusion_matrix'],
        save_path="confusion_matrix.png",
        num_classes=cfg.data.num_classes
    )

    # í´ë˜ìŠ¤ë³„ F1 Score ê·¸ë˜í”„
    plot_class_f1_scores(
        results['class_f1'],
        save_path="class_f1_scores.png"
    )

    print("\nâœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   Macro F1: {results['macro_f1']:.4f}")
    print(f"   Accuracy: {results['accuracy']:.4f}")


if __name__ == "__main__":
    main()
